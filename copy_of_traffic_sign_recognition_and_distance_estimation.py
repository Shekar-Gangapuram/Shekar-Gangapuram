# -*- coding: utf-8 -*-
"""Copy of traffic_sign_recognition_and_distance_estimation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14K_ewdHbyDyK98quqDhDTsfWxfl4ih6B

## <center><font color=darkblue> **Traffic sign recognition and distance estimation**<font></center>

## <font color=darkblue> ** Install libraries**<font>
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
import os
import cv2
import json
import random
import requests
import warnings
import skimage.io
import numpy as np
import pandas as pd
import urllib.request
import matplotlib.pyplot as plt
from ast import literal_eval as make_tuple
from google.colab.patches import cv2_imshow

# %matplotlib inline
warnings.filterwarnings("ignore")

#PROJECT_PATH = os.path.join('/content' , 'drive', 'MyDrive', 'sign_detection')
PROJECT_PATH = os.path.join('/content') #Uncomment if you work on the colab server
DATASET_PATH = os.path.join(PROJECT_PATH, 'dataset')
# ANNOTATION_PATH = os.path.join(PROJECT_PATH, 'annotation') #Uncomment if you work on the colab server
ANNOTATION_PATH = os.path.join(PROJECT_PATH, 'annotation_with_augmented')
ANNOTATION_XML_PATH = os.path.join(ANNOTATION_PATH, 'xml')
CLASS_NAMES = os.path.join(PROJECT_PATH,'traffic_distance_sensing' ,'classnames.csv')
CLASSES = os.path.join(PROJECT_PATH,'traffic_distance_sensing' ,'classes')

"""
## <font color=darkblue>** Access to the Dataset**<font>
."""

# Annotation with augmented images (for ~7000 original + ~8700 augmented images)

if not os.path.exists('annotation_with_augmented'):
  os.mkdir('annotation_with_augmented')
  !wget https://go.vicos.si/dfgaugannot -O annotation_with_augmented.zip
  !unzip annotation_with_augmented.zip -d annotation_with_augmented
  !rm annotation_with_augmented.zip

# Default annotations (for ~7000 images)
if not os.path.exists('annotation'):
    os.mkdir('annotation')
    !wget https://go.vicos.si/dfgannot -O annotation.zip
    !unzip annotation.zip -d annotation
    !rm annotation.zip

# Original and augmented images
if not os.path.exists('dataset'):
    !wget https://go.vicos.si/dfgimages -O dataset.tar.bz2
    !tar -xf dataset.tar.bz2
    !mv JPEGImages dataset
    !rm dataset.tar.bz2

"""## <font color=darkblue>** Fetch Mask RCNN**<font>


"""

if not os.path.exists('Mask_RCNN'):
  !git clone https://github.com/maxw1489/Mask_RCNN.git

!git clone https://github.com/shnnss/traffic_distance_sensing.git

os.chdir('/content/Mask_RCNN')

"""##<font color=darkblue>** Class names**<font>"""

class_names = pd.read_csv(CLASS_NAMES, sep=";", header=0)
class_names.head(5)

information = json.load(open(os.path.join(ANNOTATION_PATH, 'train.json')))

categories = pd.DataFrame(list(information['categories']))
categories.head(3)

train_images = pd.DataFrame(list(information['images']))
train_images.head(3)

annotations = pd.DataFrame(list(information['annotations']))
annotations.head(3)

import xml.etree.cElementTree as xml

if not os.path.exists(ANNOTATION_XML_PATH):
    os.mkdir(ANNOTATION_XML_PATH)

# This function is defined to generate an xml file.
def xml_generator(df, annotation_df):
    for _, row in df.iterrows():
        filename = str(row['file_name'].split('.')[0])

        annot = xml.Element("annotation")

        xml.SubElement(annot, "folder").text = "dataset"
        xml.SubElement(annot, "path").text = f"{DATASET_PATH}/{row['file_name']}"
        source = xml.SubElement(annot, "source")

        xml.SubElement(source, 'database').text = 'Unknown'

        xml.SubElement(annot, "filename").text = f"{row['file_name']}"

        size = xml.SubElement(annot, "size")
        xml.SubElement(size, "width").text = f"{row['width']}"
        xml.SubElement(size, "height").text = f"{row['height']}"
        xml.SubElement(size, "depth").text = "3"

        xml.SubElement(annot, "segmented").text = "0"

        temp_df = annotation_df[annotation_df['image_id'] == int(row['id'])]

        for i in range(temp_df.shape[0]):
            xmin, ymin, w, h = temp_df.iloc[i, 2]

            obj = xml.SubElement(annot, "object")
            xml.SubElement(obj, "name").text = f"{temp_df.iloc[i,3]}"
            xml.SubElement(obj, "title").text = categories[categories['id']==temp_df.iloc[i,3]].iloc[0, 1]
            xml.SubElement(obj, "pose").text = "unspecified"
            xml.SubElement(obj, "truncated").text = "0"
            xml.SubElement(obj, "difficult").text = "0"
            xml.SubElement(obj, "segmentation").text = f"{temp_df.iloc[i,4]}"

            bndbox = xml.SubElement(obj, "bndbox")

            xml.SubElement(bndbox, "xmin").text = f"{xmin}"
            xml.SubElement(bndbox, "ymin").text = f"{ymin}"
            xml.SubElement(bndbox, "xmax").text = f"{xmin+w}"
            xml.SubElement(bndbox, "ymax").text = f"{ymin+h}"

        tree = xml.ElementTree(annot)
        tree.write(f"{ANNOTATION_XML_PATH}/{filename}.xml")

xml_generator(train_images, annotations)

information_test = json.load(open(os.path.join(ANNOTATION_PATH, 'test.json')))

test_images = pd.DataFrame(list(information_test['images']))
test_images.head(3)

annotations_test = pd.DataFrame(list(information_test['annotations']))
annotations_test.head(3)

"""<hr>"""

xml_generator(test_images, annotations_test)

import glob

fig=plt.figure(figsize=(20,25))
i, j = 0, 0

for file_path in glob.glob(os.path.join(CLASSES, '*')):
    index = i*10+j
    image = cv2.imread(file_path, cv2.IMREAD_UNCHANGED)
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

    plt.subplot(16, 10, index + 1)
    plt.imshow(image)
    plt.axis('off')
    j +=1
    if j % 10 == 0:
        j =0
        i += 1
plt.show()

class_names[class_names['class_code']=='II-7']

"""<font color=darkblue> **Number of Classes** <font>"""

print("Number of classes is: " , len(class_names))

"""<font color=darkblue> **Number of Training Images** <font>

"""

print("Number of training images is: ", len(train_images))

"""<font color=darkblue> **Mask_RCNN required libraries** <font>"""

import mrcnn
import mrcnn.config
import mrcnn.utils
from mrcnn import visualize
from mrcnn.config import Config
from mrcnn.utils import Dataset
from mrcnn.model import MaskRCNN
from xml.etree import ElementTree

""" ## <font color=darkblue> ** Preparing the Dataset**<font>

"""

class RoadDataset(Dataset):

    def getClassIds(self,path):
        classlist = ['BG', 'I-1', 'I-1.1', 'I-10', 'I-11', 'I-13', 'I-13.1', 'I-14', 'I-15', 'I-16', 'I-17', 'I-18', 'I-19', 'I-2', 'I-2.1', 'I-20', 'I-25', 'I-27', 'I-28', 'I-28.1', 'I-29', 'I-29.1', 'I-3', 'I-30', 'I-32', 'I-34', 'I-36', 'I-37', 'I-38', 'I-39-1', 'I-39-2', 'I-39-3', 'I-4', 'I-5', 'I-5.1', 'I-5.2', 'I-8', 'I-9', 'II-1', 'II-10.1', 'II-14', 'II-17', 'II-18', 'II-19-4', 'II-2', 'II-21', 'II-22', 'II-23', 'II-26', 'II-26.1', 'II-28', 'II-3', 'II-30-10', 'II-30-30', 'II-30-40', 'II-30-50', 'II-30-60', 'II-30-70', 'II-32', 'II-33', 'II-34', 'II-35', 'II-39', 'II-4', 'II-40', 'II-41', 'II-42', 'II-42.1', 'II-43', 'II-45', 'II-45.1', 'II-45.2', 'II-46', 'II-46.1', 'II-46.2', 'II-47', 'II-47.1', 'II-48', 'II-6', 'II-7', 'II-7.1', 'II-8', 'III-1', 'III-10', 'III-105', 'III-105.1', 'III-105.3', 'III-107-1', 'III-107-2', 'III-107.1-1', 'III-107.1-2', 'III-107.2-1', 'III-107.2-2', 'III-112', 'III-113', 'III-12', 'III-120', 'III-120-1', 'III-120.1', 'III-123', 'III-124', 'III-14', 'III-14.1', 'III-15', 'III-16', 'III-18-40', 'III-18-50', 'III-18-60', 'III-18-70', 'III-2', 'III-202-5', 'III-203-2', 'III-206-1', 'III-21', 'III-23', 'III-25', 'III-25.1', 'III-27', 'III-29-30', 'III-29-40', 'III-3', 'III-30-30', 'III-33', 'III-34', 'III-35', 'III-37', 'III-39', 'III-40', 'III-42', 'III-43', 'III-45', 'III-46', 'III-47', 'III-5', 'III-50', 'III-54', 'III-59', 'III-6', 'III-64', 'III-68', 'III-74', 'III-77', 'III-78', 'III-8-1', 'III-84', 'III-84-1', 'III-85-2', 'III-85-3', 'III-85.1', 'III-86-1', 'III-86-2', 'III-87', 'III-90', 'III-90.1', 'III-90.2', 'III-91', 'IV-1', 'IV-1.1', 'IV-10', 'IV-11', 'IV-12', 'IV-12.1', 'IV-13-1', 'IV-13-2', 'IV-13-3', 'IV-13-4', 'IV-13-5', 'IV-13-6', 'IV-13.1-2', 'IV-13.1-3', 'IV-13.1-4', 'IV-16', 'IV-17', 'IV-18', 'IV-2', 'IV-20-1', 'IV-3-1', 'IV-3-2', 'IV-3-4', 'IV-3-5', 'IV-5', 'IV-6', 'VI-2.1', 'VI-3-1', 'VI-3-2', 'VI-3.1-1', 'VI-3.1-2', 'VI-8', 'VII-4', 'VII-4-1', 'VII-4-2', 'VII-4.1-1', 'VII-4.3', 'VII-4.3-1', 'VII-4.3-2', 'VII-4.4-1', 'VII-4.4-2', 'X-1.1', 'X-1.2', 'X-4', 'X-6-3']
        # Loading XML
        boxes, w, h = self.extract_boxes(path)
        # Creating one array for all masks, each on a different channel
        masks = np.zeros([h, w, len(boxes)], dtype='uint8')
        # Creating masks
        class_ids = []
        for i in range(len(boxes)):
            box = boxes[i]
            cls_id, class_title, x, y, x2, y2 = box
            row_s, row_e = y, y2
            col_s, col_e = x, x2
            masks[row_s:row_e, col_s:col_e, i] = int(cls_id)
            temp_df = categories[categories['id'] == int(cls_id)]
            if classlist.index(class_title) > 0:
                class_ids.append(classlist.index(class_title))
        _idx = np.sum(masks, axis=(0, 1)) > 0
        masks = masks[:, :, _idx]
        class_ids1 = np.asarray(class_ids, dtype='int32')[_idx]
        return np.asarray(class_ids1, dtype='int32')
    # This function is defined to loading the dataset (Train and Test set)
    def load_dataset(self, dataset_path = DATASET_PATH, is_train=True):
        for _, row in categories.iterrows():
            self.add_class('traffic_sign', row['id'], row['name'])

        loops = None
        if is_train:
            loops = train_images
        elif not is_train:
            loops = test_images
        for _, row in loops.iterrows():
            img_path = os.path.join(dataset_path, row['file_name'])
            filename = str(row['file_name'].split('.')[0])
            annPath= os.path.join(ANNOTATION_XML_PATH, f'{filename}.xml')
            gt_class_ids = self.getClassIds(annPath)
            if not (np.any(gt_class_ids > 0)):
                continue
            #     , "Unlabeled data is not supported: "\
            # + f"image_id={filename}. Make sure to remove unlabeled data"\
            # + "because negative training is not supported."

            # print("Adding File Name :",filename)
            # add to dataset
            self.add_image(
                'traffic_sign',
                image_id=row['id'],
                path=img_path,
                width=row['width'],
                height=row['height'],
                annotation= os.path.join(ANNOTATION_XML_PATH, f'{filename}.xml')
                )
    # This function is defined to extracting the bounding boxes for each traffic sign
    def extract_boxes(self, filename):
        # load and parse the file
        tree = ElementTree.parse(filename)
        # get the root of the document
        root = tree.getroot()
        # extract each bounding box and theis coordinates(xmin,xmax, ymin, ymax)
        boxes = list()
        for box in root.findall('.//object'):
            name = box.find('name').text
            title = box.find('title').text
            xmin = int(box.find('./bndbox/xmin').text)
            ymin = int(box.find('./bndbox/ymin').text)
            xmax = int(box.find('./bndbox/xmax').text)
            ymax = int(box.find('./bndbox/ymax').text)

            coors = [name, title, xmin, ymin, xmax, ymax]
            boxes.append(coors)
        # extract image dimensions
        width = int(root.find('.//size/width').text)
        height = int(root.find('.//size/height').text)
        return boxes, width, height

    # This function is defined to create masks for each bounding box using pre-trained Mask_RCNN
    def load_mask(self, image_id):
        # Getting details of the image
        info = self.image_info[image_id]
        # Definining the box file location
        path = info['annotation']
        # Loading XML
        boxes, w, h = self.extract_boxes(path)
        # Creating one array for all masks, each on a different channel
        masks = np.zeros([h, w, len(boxes)], dtype='uint8')
        # Creating masks
        class_ids = list()
        for i in range(len(boxes)):
            box = boxes[i]
            cls_id, class_title, x, y, x2, y2 = box

            row_s, row_e = y, y2
            col_s, col_e = x, x2

            masks[row_s:row_e, col_s:col_e, i] = int(cls_id)

            temp_df = categories[categories['id'] == int(cls_id)]
            class_ids.append(self.class_names.index(class_title))
        return masks, np.asarray(class_ids, dtype='int32')

    def image_reference(self, image_id):
        info = self.image_info[image_id]
        return info['path']

class RoadConfig(Config):
    """Configuration for training on the DFG dataset.
    Derives from the base Config class and overrides values specific
    to the DFG dataset.
    """

    NAME = 'road_cfg'

    # Train on 1 GPU and 1 images per GPU. We can put multiple images on each
    # GPU if the images are small. Batch size is 1 (GPUs * images/GPU).
    GPU_COUNT = 1
    IMAGES_PER_GPU = 1

    # Number of classes (including background)
    NUM_CLASSES = 1 + 200  # background + 200 signs
    RUN_EAGERLY = True
    STEPS_PER_EPOCH = 13940

config = RoadConfig()
config.display()

"""<font color=darkblue> **Preparing Training Set for Mask_RCNN** <font>



"""

train_set = RoadDataset()
train_set.load_dataset(is_train=True)

train_set.prepare()
print('Number of Training images: %d' % len(train_set.image_ids))

"""<font color=darkblue> **Preparing Test Set for Mask_RCNN** <font>"""

test_set = RoadDataset()
test_set.load_dataset(is_train=False)

test_set.prepare()
print('Number of Test images: %d' % len(test_set.image_ids))

MODEL_DIR = os.path.join(PROJECT_PATH, 'logs')
if not os.path.exists(MODEL_DIR):
    os.mkdir(MODEL_DIR)
COCO_MODEL_PATH = os.path.join(MODEL_DIR, 'mask_rcnn_coco.h5')

if not os.path.exists(COCO_MODEL_PATH):
    mrcnn.utils.download_trained_weights(COCO_MODEL_PATH)

#model =  MaskRCNN(mode='training', model_dir= MODEL_DIR, config = config)
#model.load_weights(COCO_MODEL_PATH, by_name = True, exclude = ["mrcnn_class_logits", "mrcnn_bbox_fc",  "mrcnn_bbox", "mrcnn_mask"])
#model.train(train_set, test_set, learning_rate = config.LEARNING_RATE , epochs=5, layers = 'heads')

"""<font color=darkblue> **Inference Configurations and Load the model**<font>"""

class InferenceConfig(mrcnn.config.Config):
    NAME='road_cfg'
    GPU_COUNT = 1
    NUM_CLASSES = 1+200
    IMAGES_PER_GPU = 1
    RPN_NMS_THRESHOLD = 0.01
    DETECTION_MIN_CONFIDENCE = 0.7
    DETECTION_NMS_THRESHOLD = 0.7

inference_config = InferenceConfig()

model_path = '/content/drive/MyDrive/TrafficSign/road_cfg20230620T0409/mask_rcnn_road_cfg_0005.h5'

# Recreate the model in inference mode
model = MaskRCNN(mode="inference", config=inference_config, model_dir=model_path)

# Load trained weights
print("Loading weights from ", model_path)
model.load_weights(model_path, by_name=True)

# Define all the classes with background class
def class_names_bg():
    return ['BG', *class_names['class_title'].values]

# create random colors for each bounding box
def random_color(N):
    return [tuple(np.random.randint(0, 255, 3).astype(float))for i in range(N)]

# A functoin to load the images by skimage libraries
def load_image(path):
    """Load the specified image and return a [H,W,3] Numpy array.
    """
    # Load image
    image = skimage.io.imread(path)
    # If grayscale. Convert to RGB for consistency.
    if image.ndim != 3:
        image = skimage.color.gray2rgb(image)
    # If has an alpha channel, remove it for consistency
    if image.shape[-1] == 4:
        image = image[..., :3]
    return image

"""<hr/>

<font color=purple>**test_image_id=0** (Ground_truth): <font>
"""

image_id = 0
image, image_meta, gt_class_id, gt_bbox, gt_mask = mrcnn.model.load_image_gt(test_set, inference_config, image_id)

visualize.display_instances(image, gt_bbox, gt_mask, gt_class_id, class_names_bg(),figsize=(20, 20))

"""<font color=Green>**test_image_id=0** (Prediction): <font>"""

results = model.detect([image], verbose=0)
r = results[0]

visualize.display_instances(image, r['rois'], r['masks'], r['class_ids'], class_names_bg(), r['scores'], figsize=(20, 20))

def prediction(image_path, with_distance = False):

    if isinstance(image_path, str):
        image = load_image(image_path)
    else:
        image = image_path
    height, width, _ = image.shape
    results = model.detect([image], verbose=0)

    boxes  = results[0]['rois']   # [num_instance, (y1, x1, y2, x2, class_id)] in image coordinates.
    masks  = results[0]['masks']  # masks: [height, width, num_instances]
    class_ids = results[0]['class_ids'] # [num_instances]
    scores =   results[0]['scores'] # confidence scores for each box

    masked_image = image.astype(np.uint32).copy()
    alert_flag = 0
    for i in range(boxes.shape[0]):

        y1, x1, y2, x2 = boxes[i]
        box_w = x2 - x1
        box_h = y2 - y1
        class_row = class_names[class_names['class_id'] == class_ids[i]]
        color = make_tuple(class_row.iloc[0,4])
        color_prep = np.array(color) * 255.0
        caption_position_x = x2+15
        caption_position_y = y1+(box_h//2)

        # Mask
        masked_image = visualize.apply_mask(image, masks[:, :, i], color=color)

        # Insert corresponding icon
        icon_path = os.path.join(CLASSES, f'{class_ids[i]}.png')
        if os.path.exists(icon_path):
            icon = load_image(icon_path)
            icon = skimage.transform.resize(icon, (box_h, box_w) , anti_aliasing=True)
            x_start = x2 + 10
            x_end = x_start +(x2-x1)

            if x_end > width:
                caption_position_x -= box_w - 35
                x_start = x1 - box_w - 10
                x_end = x_start +(x2-x1)

            for c in range(0, 3):
                image[y1:y2, x_start:x_end , c] = icon[:, :, c]*255.0
            caption_position_x += box_w

        # border
        cv2.rectangle(image,
                      (x1, y2),
                      (x2, y1),
                      color_prep,
                      thickness=2)

        # caption
        score = scores[i] if scores is not None else None
        label = class_names[class_names['class_id'] == class_ids[i]].iloc[0,3]
        caption = "{} {:.2f}".format(label, score) if score else label

        cv2.putText(image,
                    caption,
                    (caption_position_x, caption_position_y),
                    cv2.FONT_HERSHEY_SIMPLEX,
                    0.8,
                    color_prep,
                    thickness=2,
                    lineType=cv2.LINE_AA)

        ######_______________________Distance ________________________#####
        if with_distance and class_ids[i] in alerts.keys():
            H, bev_h, bev_w, pix_per_meter = homography(image, False)
            ego_bev_xy = bev_w // 2, bev_h  # Ego position in birdeye view

            # Draw ground midpoint markers
            ground_mid_box = (int(x1 + box_w // 2), int(y1 + box_h))

            cv2.circle(image,
                       center=ground_mid_box,
                       radius=5,
                       color=color_prep,
                       thickness=cv2.FILLED)

            ground_mid = (int(x1 + box_w // 2), int(y1 + box_h + (box_h * alerts[class_ids[i]]['rate'])))

            midpoint = np.concatenate([ground_mid, np.ones(1)])

            midpoint_warped = H @ midpoint
            midpoint_warped /= midpoint_warped[-1]
            midpoint_warped = midpoint_warped[:-1]

            ground_mid_warped = tuple(int(a) for a in midpoint_warped)

            x, y = ground_mid_warped
            delta = [x, y] - np.asarray(ego_bev_xy)
            dist_pix = np.sqrt(np.sum(delta ** 2))

            dist_meter = dist_pix / pix_per_meter

            if dist_meter <= 12:
                # Draw distance lines
                cv2.line(image,
                        ground_mid_box,
                        (image.shape[1] // 2,
                        image.shape[0]),
                        color=(255, 255, 255),
                        thickness=2)

                # Draw distance text
                x, y = ground_mid_box
                cv2.putText(image,
                            f'{dist_meter:.02f}m',
                            (x1-120, y1+box_h//2),
                            cv2.FONT_HERSHEY_SIMPLEX,
                            1,
                            color=(255, 255, 255),
                            thickness=3,
                            lineType=cv2.LINE_AA)

            # Feedback
            feedback = None
            if dist_meter > 5 and dist_meter <= 12:
                feedback = alerts[class_ids[i]]['feedback'][0]
            elif dist_meter <=5 and dist_meter >=1:
                feedback = alerts[class_ids[i]]['feedback'][1]
            elif dist_meter < 1 and len(alerts[class_ids[i]]['feedback']) == 3:
                feedback = alerts[class_ids[i]]['feedback'][2]

            if feedback is not None:
                alert_flag += 60
                cv2.putText(image,
                        feedback,
                        (width // 3, height- 300 + alert_flag),
                        cv2.FONT_HERSHEY_SIMPLEX,
                        1.2,
                        color=alerts[class_ids[i]]['color'],
                        thickness=2)


    return masked_image.astype(np.uint8)

image = prediction(os.path.join(DATASET_PATH, '0001319.jpg'))

plt.figure(figsize=(23, 23))
plt.axis('off')
plt.imshow(image)

image = prediction(os.path.join(DATASET_PATH, '0000005.jpg'))

plt.figure(figsize=(23, 23))
plt.axis('off')
plt.imshow(image)

image = prediction(os.path.join(DATASET_PATH, '0004721.jpg'))

plt.figure(figsize=(23, 23))
plt.axis('off')
plt.imshow(image)

image = prediction(os.path.join(DATASET_PATH, '0002183.jpg'))

plt.figure(figsize=(23, 23))
plt.axis('off')
plt.imshow(image)

image = prediction(os.path.join(DATASET_PATH, '0002008.jpg'))

plt.figure(figsize=(23, 23))
plt.axis('off')
plt.imshow(image)

image = prediction(os.path.join(DATASET_PATH, '0003614.jpg'))

plt.figure(figsize=(23, 23))
plt.axis('off')
plt.imshow(image)

image = prediction(os.path.join(DATASET_PATH, '0002837.jpg'))

plt.figure(figsize=(23, 23))
plt.axis('off')
plt.imshow(image)

image = prediction(os.path.join(DATASET_PATH, '0003547.jpg'))

plt.figure(figsize=(23, 23))
plt.axis('off')
plt.imshow(image)

"""## <font color=darkblue> **Evaluation**<font>


"""

def evaluation(dataset, num_samples, config, model):

    image_ids = np.random.choice(dataset.image_ids, num_samples)
    APs = []
    for image_id in image_ids:
        # Load image and ground truth data
        image, image_meta, gt_class_id, gt_bbox, gt_mask = mrcnn.model.load_image_gt(dataset,
                                                                                     config,
                                                                                     image_id,
                                                                                     use_mini_mask=False)
        molded_images = np.expand_dims(mrcnn.model.mold_image(image, config), 0)
        # Run object detection
        results = model.detect([image], verbose=0)
        r = results[0]
        # Compute evaluation metrics
        AP, precisions, recalls, overlaps = mrcnn.utils.compute_ap(gt_bbox,
                                                                   gt_class_id,
                                                                   gt_mask,
                                                                   r["rois"],
                                                                   r["class_ids"],
                                                                   r["scores"],
                                                                   r['masks'])
        APs.append(AP)

    return np.mean(APs)

mAP_training = evaluation(train_set, 10, inference_config, model)
print('mAP on training set is: ', mAP_training)

mAP_test = evaluation(test_set, 10, inference_config, model)
print('mAP on test set is: ', mAP_test)

"""#<font color=darkblue>** Distance estimation **<font>


"""

def homography(frame, auto_show=False):
    # Choose trapezoid points in frontal view
    height, width = frame.shape[:-1]
    y0, y1 = 450, height
    # y0, y1 = int(height * 0.6), height
    # int(height * 0.6)
    x_off = 50
    a = 0, y1
    b = width // 2 - x_off, y0
    c = width // 2 + x_off, y0
    d = width, y1

    trapezoid = np.expand_dims(np.asarray([a, b, c, d]), 1).astype(np.int32)

    trapezoid_img = cv2.fillPoly(np.zeros(shape=(height, width, 3)),[trapezoid], color=(0, 0, 255))

    # In real world, the trapezoid is a rectangle with the following measures
    ab_meters, ad_meters = 15 , 4

    # Measure of the bird's eye image
    pix_per_meter = 40
    bev_w = ad_meters * pix_per_meter
    bev_h = ab_meters * pix_per_meter
    xoff = 500
    dst_points = np.asarray([(xoff + 0, bev_h),
                                (xoff + 0, 0),
                                (xoff + bev_w, 0),
                                (xoff + bev_w, bev_h)])
    bev_w += (2 * xoff)

    H, mask = cv2.findHomography(trapezoid, dst_points)

    if auto_show:
        # Warp according to currently found homography
        birdeye = cv2.warpPerspective(frame, H, (bev_w, bev_h))

        alpha = 0.2
        beta = 1 - alpha

        image_show = np.uint8(trapezoid_img * alpha + frame * beta)

        ratio = image_show.shape[0] / birdeye.shape[0]
        warped_show = cv2.resize(birdeye, dsize=None, fx=ratio, fy=ratio)
        cat_show = np.concatenate([image_show, warped_show], axis=1)
        cat_show = cv2.resize(cat_show, dsize=None, fx=0.4, fy=0.4)
        cv2_imshow(cat_show)
        return

    return H, bev_h, bev_w, pix_per_meter

frame = load_image(os.path.join(DATASET_PATH, '0000002.jpg'))

homography(frame, True)

alerts = {
    38:{'rate':3,'color': (255, 255, 255),
        'feedback':[
            'You are getting closer to the give way traffic sign!!', 'WAIT !!! and pay attention to the major road!! '
        ]}, # Give way to major road
    187:{'rate':0.1,'color': (255, 255, 255), 'feedback':[
        'Island marker!', 'Island marker! in less than 5 meters'
    ]}, # Traffic island marker
    100:{'rate':3,'color': (255, 255, 255), 'feedback':[
        'you are getting closer to the Emergency station', 'Emergency station on less than 5 meters'
    ]}, # Emergency station
    64:{'rate':4,'color': (255, 255, 255), 'feedback':[
        'Danger of crashing with the bicycle', 'Decrease the speed!!!! Pay attention to Bike Path!!!', 'You are safe now, if you want, increase your speed. :)'
    ]}, # Bike path
    137:{'rate':4,'color': (255, 0, 0), 'feedback':[
        'Danger of crashing with the Pedestrian', 'Decrease the speed!!!! Pay attention to Pedestrian!!!', 'You are safe now, if you want, increase your speed.'
    ]}, # Pedestrian crossing
    44:{'rate':3,'color': (255, 0, 0), 'feedback':[
        'You are getting closer to the stop sign', 'OK decrease the speed and stop the car!', 'STOP STOP STOP!!! POOOOOOFFFF (you are crashed)'
    ]}, # Stop
    124:{'rate':4,'color': (255, 255, 255), 'feedback':[
        'there is a parking lot in less than 10 meters', 'there is a parking lot very close to you'
    ]}, # Parking
    185:{'rate': 1.1,'color': (255, 255, 255), 'feedback':[
        'Hazard Marker', 'pay attention to the change in the direction of travel'
    ]}, # Hazard marker
    59:{'rate': 2.2,'color': (255, 255, 255), 'feedback':[
        'Give way to oncoming traffic in the road', 'Give way to oncoming traffic!'
    ]}, # Give way to oncoming traffic
    40:{'rate': 3,'color': (255, 255, 255), 'feedback':[
        'Road are closed for bikes!', 'Road are closed for bikes!!'
    ]}, # Road closed to bicycles
    135: {'rate': 3.9,'color': (255, 255, 255), 'feedback':[
        'There is a bus station in front of you, danger of crashing!', 'BUS station close to you, decrease your speed buddy!'
    ]}, # Bus stop
    55: {'rate': 2,'color': (200, 0, 0), 'feedback':[
        'if you are driving fast, decrease the speed!', 'The maximum speed is 50!'
    ]}, # Maximum speed 50
    53: {'rate': 4,'color': (200, 0, 0), 'feedback':[
        'if you are driving fast, decrease the speed!', 'The maximum speed is 30!'
    ]}, # Maximum speed 30
}

image_path = os.path.join(DATASET_PATH, '0000005.jpg')

plt.figure(figsize=(20,20))
plt.axis('off')
plt.imshow(prediction(image_path, with_distance=True))

image_path = os.path.join(DATASET_PATH, '0002837.jpg')

plt.figure(figsize=(20,20))
plt.axis('off')
plt.imshow(prediction(image_path, with_distance=True))

import imageio
import cv2

video_path = '/content/sample_data/VID_20230619_153835.mp4'
frames_path = '/content/frames'
destination = os.path.join(PROJECT_PATH, 'final.mp4')

vid = imageio.get_reader(video_path)

for num, frame in enumerate(vid.iter_data()):
    image = prediction(frame, with_distance=True)
    if not os.path.exists(frames_path):
        os.mkdir(frames_path)

    skimage.io.imsave(os.path.join(frames_path, f'{num}.jpeg'), image)

def make_video(outvid, images=None, fps=30, size=None,
               is_color=True, format="FMP4"):
    """
    Create a video from a list of images.

    @param      outvid      output video
    @param      images      list of images to use in the video
    @param      fps         frame per second
    @param      size        size of each frame
    @param      is_color    color
    @param      format      see http://www.fourcc.org/codecs.php
    @return                 see http://opencv-python-tutroals.readthedocs.org/en/latest/py_tutorials/py_gui/py_video_display/py_video_display.html

    The function relies on http://opencv-python-tutroals.readthedocs.org/en/latest/.
    By default, the video will have the size of the first image.
    It will resize every image to this size before adding them to the video.
    """
    from cv2 import VideoWriter, VideoWriter_fourcc, imread, resize
    fourcc = VideoWriter_fourcc(*format)
    vid = None
    for image in images:
        if not os.path.exists(image):
            raise FileNotFoundError(image)
        img = imread(image)
        if vid is None:
            if size is None:
                size = img.shape[1], img.shape[0]
            vid = VideoWriter(outvid, fourcc, float(fps), size, is_color)
        if size[0] != img.shape[1] and size[1] != img.shape[0]:
            img = resize(img, size)
        vid.write(img)
    vid.release()
    return vid

import glob
import os

images = list(glob.iglob(os.path.join(frames_path, '*.*')))
# Sort the images by integer index
images = sorted(images, key=lambda x: int(os.path.split(x)[1][:-5]))

make_video(destination, images, 30)